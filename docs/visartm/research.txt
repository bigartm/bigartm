For researchers
====================

Object model
-----------------------------------
**Dataset** is a collection of **documents**. It contains vocabulary (set of **terms**) and **documents**.

Documents technically are multisets or lists of terms. Also they can contain *raw text*.

Each term belongs to **modality**. If **modalities** are undefined, all terms belongs to **modality** @default_class.

**ArtmModel** is topic model.

Dataset uploading
-----------------------------------

So, to get started you need to download the dataset. Create a project folder, the folder data, it datasets folder in her folder for each dataset.
The UCI dataset'a folder, create a folder in which you will need to put the two files with the description of vocabulary words and bags. This is enough to
it was possible to build a model, but you can not see the text. Therefore, essli you have all the documents in a tabular format, download them to a folder documents, calling numbers with the unit with the extension .txt. Download the "Index Files" in "word_index" folder. Finally, create a file **documents.json** with meta-data of all documents. As a result, you should get this file structure:

.. code-block:: bash 

   artmonline
		data
			datasets
				dataset1
					UCI
						docword.dataset1.txt
						vocab.dataset1.txt
					documents
						1.txt
						2.txt
						.....
					word_index
						1.txt
						2.txt
						.....
					documents.json
							
Now see exactly all the files. vocab.dataset1.txt file (instead dataset1 put the name of your dataset'a, as should nazyatsya dataset'a folder)
It must contain the same number of rows as there are words in the dictionary. Each line must be one word. If you have a model,
then on each line, after the words, separated by a space, write the modalities. There are no gaps in terms not be doolzhno. If you have any N-gram, use the underscore.

docword.dataset1.txt file contains words bags. The first line contains the number of documents, the second - the number of terms (must match the number of rows in the previous file), in the third - the number of lines in the file (the first three are not considered). In all other lines written by three numbers - the number of the document, the word number, the number of occurrences. It is important that the words were in line with numbers their position in the dictionary (numbered from one).

.. code-block:: bash 

   D
   W
   NNZ
   docID wordID count
   docID wordID count
   ...
   docID wordID count

IMPORTANT! Entries must be ordered by docID.

Examples can be seen on https://archive.ics.uci.edu/ml/machine-learning-databases/bag-of-words/readme.txt

It is important that each document was at least one term.

The documents in the documents folder - just text.

The files in the folder word_index need only to highlight the words while rendering. Not all words are terms, words in a document
not necessarily in the initial form, sometimes terms - is n-grams. Therefore, you must specify where in the document is what terms.
Files in a folder word_index consists of several (according to the number of terms) of rows, each containing four integers

.. code-block:: bash 

   line start_pos length term_id
   
**line** - this line number (numbered from the unit), start_pos - the beginning of the term in this line (nkmeratsiya from scratch!), length - length of the entry, **term_id** - term number in the same numbering that docword file.

Finally, documents.json - is JSON-dictionary with all the information about the documents. His keys - a number of documents,
and the values of a dictionary with the following fields:

.. code-block:: bash 

   title
   snippet
   url
   time
   
Time must be an integer in UNIX TIMESTAMP format.

Now, when you put all the necessary data, you can import them. To do this, select the interface ARTM Online punkkt Datasets at the top, then right click on the Create new dataset ... From Dataset name list, select the name of your dataset'a (system finds it automatically, if you pomestilii it in the correct folder). Now correctly you set the following checkboxes (this is important!):

* UCI vocab and docword provided - including mandatory
* Documents.json provided - include if there documents.json file
* Text provided - include if there is text files
* Word index provided - include if there word_index files
* Time provided provided - include, if you have time
 
Click on the Create button and wait until the dataset is imported.

Now you have a folder batches and models in the folder with the DataSet. In batches folder contains data for yhodnye BigARTM. Only use them!

Model creation
----------------------------------------------------------------------------


For a quick start, try the built-builders models.
Go to Datasets, select the newly created dataset, right click Create new model ...
Select Flat or Hierarchical, set the desired number, and then click Create.

Let us consider another option. You yourself will write
script for python on, building model,
and ARTM Online will process the output of your script and build the rendering.

Please go to the Datasets, select the newly chozdanny dataset, right click Create new model ... Now select Empty tab and click create.
You will have a folder in the models folder. There is a sample script that reads data from batches, builds a simple model, and writes in the form of a matrix
pandas.dataframe'ov in a folder with the model. Of course, you can put your script anywhere important to read and write in the correct places.

So, your script should save your work BigARTM'a. This matrix of theta and phi. Keep them using to_pickle () in the theta and phi files.

If you are building N-tier hierarchy using hARTM, it is necessary to save more files psi1, psi2, ..., psi (N-1).

When kept all of the matrix, go to the page model (it can be accessed from the dataset or page, look for the right list of models
or from your profile page). There, click on the Reload Model. Wait until the import will happen. Now you can look at your model fizualizatsiyu.

You can work well. Create a model, make changes in it and restart.
Sozdaat several possible models (e.g., with different parameters). They will all be available odnovremennl.
 
