Introduction
============

.. warning::

   Please note that this is a beta version of the BigARTM library
   which is still undergoing final testing before its official release.
   Should you encounter any bugs, lack of functionality or
   other problems with our library, please let us know immediately.
   Your help in this regard is greatly appreciated.

This is the documentation for the BigARTM library.
BigARTM is a tool to infer `topic models`_,
based on a novel technique called
`Additive Regularization of Topic Models`_.
This technique effectively builds multi-objective models
by adding the weighted sums of regularizers to the optimization criterion.
BigARTM is known to combine well
very different objectives, including sparsing, smoothing, topics decorrelation and many others.
Such combinations of regularizers significantly improves
several quality measures at once almost without any loss of the perplexity.

**Online**.
BigARTM never stores the entire text collection
in the main memory. Instead the collection is split into
small chunks called 'batches', and BigARTM always loads a limited
number of batches into memory at any time.

**Parallel**.
BigARTM can concurrently process several batches,
and by doing so it substantially improves the throughput
on multi-core machines. The library hosts all computation
in several threads withing a single process,
which enables efficient usage of shared memory across application threads.

**Extensible API**.
BigARTM comes with an API in Python,
but can be easily extended for all other languages
that have an implementation of `Google Protocol Buffers`_.

**Cross-platform**.
BigARTM is known to be compatible with gcc,
clang and the Microsoft
compiler (VS 2012). We have tested our library on Windows, Ubuntu
and Fedora.

**Open source**.
BigARTM is released under the `New BSD License`_.

.. _Additive Regularization of Topic Models: http://www.machinelearning.ru/wiki/images/1/1f/Voron14aist.pdf
.. _topic models: http://en.wikipedia.org/wiki/Topic_model
.. _Google Protocol Buffers: https://code.google.com/p/protobuf/
.. _New BSD license: http://opensource.org/licenses/BSD-3-Clause

=========================================

**Acknowledgements**.
BigARTM project is supported by Russian Foundation for Basic Research (grants 14-07-00847, 14-07-00908, 14-07-31176),
Skolkovo Institute of Science and Technology (project 081-R), Moscow Institute of Physics and Technology.

.. image:: _images/sponsors_RFBR.png
   :alt: RFBR
   :target: http://www.rfbr.ru/rffi/eng/about

.. image:: _images/sponsors_skoltech.png
   :alt: Skoltech
   :target: http://www.skoltech.ru/en

.. image:: _images/sponsors_MIPT.png
   :alt: MIPT
   :target: http://mipt.ru/en/

**Partners**

.. image:: _images/partners_ap-logo.png
   :alt: antiplagian
   :target: http://https://confluence.ap-team.ru/display/research/Antiplagiat+Research+Home/

.. image:: _images/partner_bBridge.png
   :alt: bBridge
   :target: http://bbridge.net/
